# Phase 4 Testing Runbook

## Quick Start: Run All Tests in 2 Minutes

```bash
# Terminal: Run Phase 4 test suite
cd c:\Users\nisha\Projects\tcs_ai\course_ai_agent
pytest tests/test_phase_4_web_search.py -v

# Expected Output:
# ===== test session starts =====
# tests/test_phase_4_web_search.py::TestSearchTools::test_search_tools_initialization PASSED
# tests/test_phase_4_web_search.py::TestSearchTools::test_search_result_format PASSED
# ... (28 more tests)
# ===== 30 passed in 15.43s =====
```

## Test Categories

### Category 1: Search Tools (8 tests)

**Purpose:** Verify SearchResult format, tool initialization, fallback chain

```
test_search_tools_initialization
â”œâ”€ Verify TavilySearchTool initializes
â”œâ”€ Verify DuckDuckGoSearchTool initializes
â”œâ”€ Verify SerpAPISearchTool initializes
â””â”€ Verify WebSearchToolchain initializes

test_search_result_format
â”œâ”€ Create SearchResult with title, url
â”œâ”€ Check dataclass fields exist
â”œâ”€ Verify relevance_score defaults to 0.8
â””â”€ Confirm source attribute exists

test_toolchain_deduplication
â”œâ”€ Create 10 results, 4 duplicates
â”œâ”€ Call toolchain.deduplicate_results()
â”œâ”€ Verify returns 6 unique (by URL)
â””â”€ Check first occurrence kept

test_toolchain_fallback_chain
â”œâ”€ Mock all tools to fail
â”œâ”€ Call toolchain.search()
â”œâ”€ Verify tries Tavily first
â”œâ”€ Verify tries DuckDuckGo second
â”œâ”€ Verify tries SerpAPI third
â””â”€ Verify returns empty list when all fail

[4 more tests: batch search, singleton pattern, search stats, history]
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestSearchTools -v
```

### Category 2: Output Schema (8 tests)

**Purpose:** Validate WebSearchAgentOutput structure, serialization, confidence

```
test_web_search_output_creation
â”œâ”€ Create empty WebSearchAgentOutput
â”œâ”€ Set summary, confidence_score
â”œâ”€ Verify all fields initialized
â””â”€ Check defaults

test_empty_search_output
â”œâ”€ Call WebSearchAgentOutput.empty_search()
â”œâ”€ Verify confidence_score = 0.0
â”œâ”€ Verify result_count = 0
â”œâ”€ Verify is_usable() returns False

test_confidence_thresholds
â”œâ”€ Create output with confidence 0.85
â”œâ”€ Check is_high_confidence() â†’ True
â”œâ”€ Create output with confidence 0.45
â”œâ”€ Check is_high_confidence() â†’ False

test_source_link_dataclass
â”œâ”€ Create SourceLink with URL, title
â”œâ”€ Set source_type = "institution"
â”œâ”€ Verify relevance_score attribute
â””â”€ Check accessed_at timestamp

test_recommended_module_dataclass
â”œâ”€ Create RecommendedModule
â”œâ”€ Set title, description, topics
â”œâ”€ Verify source_urls list exists
â””â”€ Check estimated_hours

test_to_dict_serialization
â”œâ”€ Create WebSearchAgentOutput
â”œâ”€ Call output.to_dict()
â”œâ”€ Verify returns dict (JSON-serializable)
â”œâ”€ Check all fields present

test_from_dict_deserialization
â”œâ”€ Create dict representation
â”œâ”€ Call WebSearchAgentOutput.from_dict()
â”œâ”€ Verify object recreated
â”œâ”€ Check field values match

test_output_str_representation
â”œâ”€ Create WebSearchAgentOutput
â”œâ”€ Call str(output)
â”œâ”€ Verify readable format
â””â”€ Check confidence visible
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestWebSearchAgentOutput -v
```

### Category 3: Web Search Agent (7 tests)

**Purpose:** Verify agent logic, query generation, LLM synthesis

```
test_agent_initialization
â”œâ”€ Create WebSearchAgent()
â”œâ”€ Verify toolchain exists
â”œâ”€ Check reset_agent() works
â””â”€ Confirm singleton pattern

test_search_query_generation
â”œâ”€ Call _generate_search_queries()
â”œâ”€ Verify returns 3 queries
â”œâ”€ Check queries are contextual
â””â”€ Confirm uses audience_category + depth

test_agent_full_pipeline
â”œâ”€ Create ExecutionContext
â”œâ”€ Call agent.run(context)
â”œâ”€ Verify returns WebSearchAgentOutput
â”œâ”€ Check execution_time_ms > 0

test_agent_with_no_results
â”œâ”€ Call agent with query: "xyz_nonexistent_course_123"
â”œâ”€ Verify handles gracefully
â”œâ”€ Check returns empty_search()
â””â”€ Confirm soft failure (no exception)

test_llm_synthesis
â”œâ”€ Create mock results
â”œâ”€ Call _synthesize_results()
â”œâ”€ Verify returns structured output
â””â”€ Check summary + topics populated

test_synthesis_with_fallback
â”œâ”€ Mock LLM to fail
â”œâ”€ Call _synthesize_results()
â”œâ”€ Verify falls back to _simple_result_extraction()
â”œâ”€ Ensure still returns valid output
â””â”€ Check low confidence_score (0.3)

test_agent_search_budget
â”œâ”€ Create agent with search_budget = 1
â”œâ”€ Call agent.run()
â”œâ”€ Verify only 1 query executed
â””â”€ Confirm multiple queries if budget > 1
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestWebSearchAgent -v
```

### Category 4: Failure Resilience (5 tests)

**Purpose:** Verify agent handles errors gracefully, doesn't crash orchestrator

```
test_network_error_handling
â”œâ”€ Mock search to raise RequestError
â”œâ”€ Call agent.run()
â”œâ”€ Verify doesn't raise (catches exception)
â”œâ”€ Check returns empty_search() or fallback
â””â”€ Confirm orchestrator can continue

test_llm_service_failure
â”œâ”€ Mock llm_service to raise exception
â”œâ”€ Call _synthesize_results()
â”œâ”€ Verify falls back to simple extraction
â”œâ”€ Check output still valid
â””â”€ Verify confidence_score low (0.3)

test_timeout_handling
â”œâ”€ Mock search to timeout (5s)
â”œâ”€ Set timeout threshold
â”œâ”€ Call agent.run()
â”œâ”€ Verify catches timeout exception
â””â”€ Check returns empty or next tool

test_all_tools_fail
â”œâ”€ Mock Tavily, DuckDuckGo, SerpAPI all fail
â”œâ”€ Call agent.run()
â”œâ”€ Verify tries all three
â”œâ”€ Check returns empty_search()
â””â”€ Confirm no unhandled exception

test_malformed_llm_response
â”œâ”€ Mock LLM to return invalid JSON
â”œâ”€ Call _parse_synthesized_output()
â”œâ”€ Verify graceful degradation
â”œâ”€ Check falls back to simple extraction
â””â”€ Confirm returns valid output
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestFailureResilience -v
```

### Category 5: Provenance Tracking (4 tests)

**Purpose:** Verify attribution, timestamps, source tracking

```
test_tool_attribution
â”œâ”€ Execute search that uses Tavily
â”œâ”€ Check output.tool_used = "tavily"
â”œâ”€ Execute search that uses DuckDuckGo
â”œâ”€ Check output.tool_used = "duckduckgo"
â””â”€ Execute search that uses SerpAPI
   â””â”€ Check output.tool_used = "serpapi"

test_execution_timing
â”œâ”€ Record start time
â”œâ”€ Call agent.run()
â”œâ”€ Record end time
â”œâ”€ Verify output.execution_time_ms in expected range
â””â”€ Check timing >= 50ms (network latency)

test_fallback_tracking
â”œâ”€ Mock primary tool to fail
â”œâ”€ Call agent.run()
â”œâ”€ Check output.fallback_used = True
â””â”€ Verify tool_used = fallback tool

test_source_link_urls
â”œâ”€ Run agent.run()
â”œâ”€ Get output.source_links
â”œâ”€ Verify each has valid URL (https://)
â”œâ”€ Check no empty URLs
â””â”€ Confirm titles present
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestProvenance -v
```

### Category 6: Full Integration (1 test)

**Purpose:** Verify entire phase with realistic input

```
test_phase_4_end_to_end
â”œâ”€ Create realistic UserInput:
â”‚  â”œâ”€ course_title: "Machine Learning Fundamentals"
â”‚  â”œâ”€ audience_category: "working_professionals"
â”‚  â”œâ”€ depth_requirement: "overview_level"
â”‚  â””â”€ learning_mode: "self_paced"
â”œâ”€ Create ExecutionContext
â”œâ”€ Call RetrievalAgent.run() (Phase 3)
â”œâ”€ Call WebSearchAgent.run() (Phase 4) â† New
â”œâ”€ Verify context.retrieved_documents populated
â”œâ”€ Verify context.web_search_results populated
â”œâ”€ Call ModuleCreationAgent.run()
â”œâ”€ Verify CourseOutlineSchema returned
â”œâ”€ Check outline has:
â”‚  â”œâ”€ module_list (from all sources)
â”‚  â”œâ”€ learning_objectives (merged)
â”‚  â””â”€ references (both internal + external)
â””â”€ Confirm total_duration_hours > 0
```

**Run individually:**
```bash
pytest tests/test_phase_4_web_search.py::TestPhase4Integration::test_phase_4_end_to_end -v
```

## Manual Testing Guide

### Test 1: Verify Basic Search Tool

**Goal:** Confirm search tool initializes and returns structured results

```python
# File: test_manual_phase4.py

import asyncio
from tools.web_search_tools import WebSearchToolchain, SearchResult

def test_search_toolchain():
    """Verify toolchain returns valid SearchResult objects"""
    
    toolchain = WebSearchToolchain()
    print(f"âœ… Toolchain initialized")
    
    # Search for common course
    results, tool = toolchain.search("Python programming course", max_results=3)
    
    print(f"ðŸ“Š Search Results:")
    print(f"  Tool used: {tool}")
    print(f"  Results found: {len(results)}")
    
    for i, result in enumerate(results, 1):
        print(f"\n  [{i}] {result.title}")
        print(f"      URL: {result.url}")
        print(f"      Snippet: {result.snippet[:80]}...")
        print(f"      Relevance: {result.relevance_score}")
        
        # Validate structure
        assert hasattr(result, 'title'), "Missing title"
        assert hasattr(result, 'url'), "Missing url"
        assert hasattr(result, 'snippet'), "Missing snippet"
        assert isinstance(result.relevance_score, float), "relevance_score not float"
        assert 0.0 <= result.relevance_score <= 1.0, "relevance_score out of range"
    
    print(f"\nâœ… All results have correct structure")
    print(f"âœ… Relevance scores valid")
    
    # Test deduplication
    results_dup = results + results
    unique = toolchain.deduplicate_results(results_dup)
    print(f"\nðŸ“Š Deduplication:")
    print(f"  Input: {len(results_dup)} (with duplicates)")
    print(f"  Output: {len(unique)} (unique)")
    assert len(unique) == len(results), "Deduplication failed"
    print(f"âœ… Deduplication works correctly")

if __name__ == "__main__":
    test_search_toolchain()
```

**Run:**
```bash
python test_manual_phase4.py
```

**Expected Output:**
```
âœ… Toolchain initialized
ðŸ“Š Search Results:
  Tool used: tavily
  Results found: 3

  [1] Python Programming Course - Beginner to Advanced
      URL: https://...
      Snippet: "Learn Python from basics to advanced...
      Relevance: 0.95

  [2] Python Fundamentals Course
      URL: https://...
      Snippet: "Master Python for data science...
      Relevance: 0.87

  [3] Python Programming Tutorial
      URL: https://...
      Snippet: "Complete guide to Python...
      Relevance: 0.85

âœ… All results have correct structure
âœ… Relevance scores valid

ðŸ“Š Deduplication:
  Input: 6 (with duplicates)
  Output: 3 (unique)
âœ… Deduplication works correctly
```

### Test 2: Verify WebSearchAgent

**Goal:** Test agent query generation and full pipeline

```python
# File: test_manual_agent.py

import asyncio
from agents.web_search_agent import WebSearchAgent
from schemas.execution_context import ExecutionContext
from schemas.user_input import UserInputSchema

async def test_web_search_agent():
    """Test agent query generation and search"""
    
    agent = WebSearchAgent()
    print(f"âœ… Agent initialized")
    
    # Create test input
    user_input = UserInputSchema(
        course_title="Machine Learning Applications",
        audience_category="working_professionals",
        depth_requirement="implementation_level",
        learning_mode="project_based",
        session_id="test_manual"
    )
    
    context = ExecutionContext(
        user_input=user_input,
        session_id="test_manual"
    )
    
    print(f"\nðŸ“ Input:")
    print(f"  Title: {user_input.course_title}")
    print(f"  Audience: {user_input.audience_category}")
    print(f"  Depth: {user_input.depth_requirement}")
    
    # Generate queries
    queries = await agent._generate_search_queries(user_input.course_title)
    print(f"\nðŸ” Generated Queries ({len(queries)}):")
    for i, q in enumerate(queries, 1):
        print(f"  [{i}] {q}")
    
    assert len(queries) == 3, "Expected 3 queries"
    print(f"âœ… Query generation works")
    
    # Run full agent
    print(f"\nâš™ï¸ Running full agent pipeline...")
    output = await agent.run(context)
    
    print(f"\nðŸ“Š Agent Output:")
    print(f"  Search Summary: {output.search_summary[:100]}...")
    print(f"  Key Topics: {output.key_topics_found}")
    print(f"  Confidence: {output.confidence_score}")
    print(f"  Tool Used: {output.tool_used.value}")
    print(f"  Results Count: {output.result_count}")
    print(f"  Execution Time: {output.execution_time_ms}ms")
    
    # Validate output
    assert output.search_query is not None, "Missing search_query"
    assert output.confidence_score >= 0.0, "Invalid confidence"
    assert output.confidence_score <= 1.0, "Invalid confidence"
    assert output.result_count >= 0, "Invalid result count"
    
    print(f"\nâœ… Agent output valid")
    print(f"âœ… Confidence score: {output.confidence_score}")
    
    if output.is_high_confidence():
        print(f"âœ… Result is HIGH CONFIDENCE")
    else:
        print(f"âš ï¸ Result is LOW CONFIDENCE")
    
    # Check URLs
    if output.source_links:
        print(f"\nðŸ“š Source Links ({len(output.source_links)}):")
        for link in output.source_links[:3]:
            print(f"  - {link.title}")
            print(f"    {link.url[:60]}...")
    
    # Check recommended modules
    if output.recommended_modules:
        print(f"\nðŸ“š Recommended Modules ({len(output.recommended_modules)}):")
        for mod in output.recommended_modules[:2]:
            print(f"  - {mod.title}")
            print(f"    Description: {mod.description[:60]}...")

if __name__ == "__main__":
    asyncio.run(test_web_search_agent())
```

**Run:**
```bash
python test_manual_agent.py
```

**Expected Output:**
```
âœ… Agent initialized

ðŸ“ Input:
  Title: Machine Learning Applications
  Audience: working_professionals
  Depth: implementation_level

ðŸ” Generated Queries (3):
  [1] Machine Learning Applications syllabus curriculum
  [2] Machine Learning Applications working_professionals course outline
  [3] Machine Learning Applications learning objectives implementation_level

âœ… Query generation works

âš™ï¸ Running full agent pipeline...

ðŸ“Š Agent Output:
  Search Summary: Based on public sources, machine learning applications courses typically...
  Key Topics: ['supervised learning', 'neural networks', 'model deployment', ...]
  Confidence: 0.82
  Tool Used: tavily
  Results Count: 12
  Execution Time: 1234ms

âœ… Agent output valid
âœ… Confidence score: 0.82
âœ… Result is HIGH CONFIDENCE

ðŸ“š Source Links (12):
  - Stanford CS229: Machine Learning
    https://cs229.stanford.edu/...
  - Coursera ML Specialization
    https://www.coursera.org/...
  - Fast.ai Practical Deep Learning
    https://fast.ai/...

ðŸ“š Recommended Modules (5):
  - Supervised Learning Fundamentals
    Description: Classification, regression, and evaluation metrics...
  - Deep Learning & Neural Networks
    Description: Neural network architectures, training techniques...
```

### Test 3: Verify Orchestrator Integration

**Goal:** Test Phase 3 + Phase 4 together

```python
# File: test_orchestrator_phase4.py

import asyncio
from agents.orchestrator import CourseOrchestratorAgent
from schemas.user_input import UserInputSchema

async def test_orchestrator_with_web_search():
    """Test orchestrator with both Retrieval + WebSearch"""
    
    orchestrator = CourseOrchestratorAgent()
    print(f"âœ… Orchestrator initialized")
    
    user_input = UserInputSchema(
        course_title="Data Science Fundamentals",
        audience_category="beginners",
        depth_requirement="overview_level",
        learning_mode="blended",
        session_id="test_orch"
    )
    
    print(f"\nðŸ“ Input:")
    print(f"  Title: {user_input.course_title}")
    print(f"  Audience: {user_input.audience_category}")
    
    print(f"\nâš™ï¸ Running orchestrator with Phase 3 + Phase 4...")
    
    outline = await orchestrator.run(user_input, "test_orch")
    
    print(f"\nðŸ“Š Orchestrator Output:")
    print(f"  Course Title: {outline.course_title}")
    print(f"  Total Duration: {outline.total_duration_hours}h")
    print(f"  Modules: {len(outline.module_list)}")
    print(f"  Learning Outcomes: {len(outline.learning_outcomes)}")
    
    print(f"\nâœ… Orchestrator completed successfully")
    print(f"âœ… Both Retrieval + WebSearch executed")
    
    # Check that we have references from both
    internal_refs = [r for r in outline.references if r.get("source") == "internal"]
    external_refs = [r for r in outline.references if r.get("source") == "external"]
    
    print(f"\nðŸ“š References:")
    print(f"  Internal (Phase 3): {len(internal_refs)}")
    print(f"  External (Phase 4): {len(external_refs) }")
    print(f"  Total: {len(outline.references)}")

if __name__ == "__main__":
    asyncio.run(test_orchestrator_with_web_search())
```

**Run:**
```bash
python test_orchestrator_phase4.py
```

## Troubleshooting

### Issue: Tests fail with "API key not found"

**Cause:** Tavily API key not set

**Solution:**
```bash
# Either set environment variable
set TAVILY_API_KEY=your_key_here

# Or tests will use mock implementation (recommended for testing)
# Tests include mock_search() that works without API key
```

### Issue: "test_phase_4_web_search.py not found"

**Cause:** Still in wrong directory

**Solution:**
```bash
# Navigate to project root
cd c:\Users\nisha\Projects\tcs_ai\course_ai_agent

# Verify file exists
dir tests/test_phase_4_web_search.py

# Run from project root
pytest tests/test_phase_4_web_search.py -v
```

### Issue: "ModuleNotFoundError: No module named 'tavily'"

**Cause:** Tavily SDK not installed

**Solution:**
```bash
# Install tavily-python
pip install tavily-python

# For testing, this is optional - mock implementation works
```

### Issue: "duckduckgo_search not installed"

**Cause:** DuckDuckGo package optional

**Solution:**
```bash
# Install optional package
pip install duckduckgo-search

# Or tests will skip DuckDuckGo tests gracefully
```

### Issue: Tests timeout (> 30 seconds)

**Cause:** Network latency or LLM API slow

**Solution:**
```bash
# Run with longer timeout
pytest tests/test_phase_4_web_search.py -v --timeout=60

# Or run specific test (faster)
pytest tests/test_phase_4_web_search.py::TestSearchTools -v
```

## Summary

**All 30 tests validate:**
- âœ… Search tools work correctly
- âœ… Output schema is properly structured
- âœ… Agent generates contextual queries
- âœ… Multi-tool fallback executes correctly
- âœ… LLM synthesis works (with fallback)
- âœ… Failures are handled gracefully
- âœ… Provenance is tracked completely
- âœ… Full orchestrator integration works
- âœ… Non-breaking integration with Phase 3
- âœ… System doesn't crash on errors

**Success Criteria:**
```
pytest tests/test_phase_4_web_search.py -v
===== 30 passed in ~15s =====

pytest tests/ -v  
===== 75 passed in ~60s =====
  (Phase 2: 20, Phase 3: 25, Phase 4: 30)
```

